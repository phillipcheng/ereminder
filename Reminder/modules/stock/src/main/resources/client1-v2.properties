#task mgr config
hadoop.hdfs.default.name=hdfs://localhost:19000
hdfs.task.folder=/reminder/task
hdfs.crawleditem.folder=/reminder/items
#comment out following 2 lines to debug within the same jvm, uncomment them for local hadoop running
#hadoop.job.tracker=127.0.0.1:9001
#yarn.application.classpath=/reminder/lib/
crawl.task.per.mapper=10
task.mapper.class:org.cld.datacrawl.hadoop.CrawlTaskMapper
mapreduce.jobhistory.address=locahost:10020
mapreduce.jobhistory.webapp.address=localhost:19888
mapreduce.jobhistory.intermediate-done-dir=/mr-history/tmp
mapreduce.jobhistory.done-dir=/mr-history/done
dfs.replication=1
mapred.textoutputformat.separator=,
mapreduce.task.timeout=0
mapreduce.job.split.metainfo.maxsize=-1
mapreduce.map.speculative=false
mapreduce.job.user.classpath.first=true
yarn.scheduler.minimum-allocation-mb=128

#crawl mgr config
use.proxy=false
#houston proxy
proxy.ip=16.85.88.10
proxy.port=8080

#time out for remote request, fetch page, invoke ws request, etc, in second unit
time.out=40

# of timeout retry for each link = retry.num * max.loop * wait.time
retry.num=1
max.loop=5
wait.time=1000

#data store manager: a list of hibernate, hbase, hdfs, nothing, 1st is default
crawl.ds.manager=hbase, hdfs
#hibernate ds manager config
#crawl.db.connection.url=jdbc:h2:tcp://localhost/~/reminderprd;LOCK_MODE=0
#hbase-site.xml should be put under class path if using hbase ds manager

#result data manager, crawled to hdfs then loaded to data store, like hive, etc
#10000 for hiveserver2
#10001 for sparksql-thriftserver
big.dm.driver=org.apache.hive.jdbc.HiveDriver
big.dm.url=jdbc:hive2://192.85.247.104:10000/default
big.dm.user=dbadmin
big.dm.pass=password

small.dm.driver=com.mysql.jdbc.Driver
small.dm.url=jdbc:mysql://192.85.247.105:3306/stock
small.dm.user=mysql
small.dm.pass=mysql
#
enable.stat=true
#
#ws.main.url=http://54.187.167.132:8080/crbookws/services/crbookrs #amazon
#ws.main.url=http://localhost:8080/crbookws/services/crbookrs
#
#plugin.jar=

#product definition
product.type=default
default.entity.impl=org.cld.datastore.entity.Product
default.handler.impl=org.cld.pagea.general.DefaultHandler

#task type definition
task.type=org.cld.datacrawl.task.BrowseCategoryTaskConf,org.cld.datacrawl.task.BrowseDetailTaskConf,org.cld.datacrawl.task.BrowseProductTaskConf,org.cld.datacrawl.task.TestTaskConf,org.cld.datacrawl.task.InvokeTaskTaskConf

org.cld.datacrawl.task.BrowseCategoryTaskConf.entity=org.cld.datacrawl.task.BrowseCategoryTaskConf
org.cld.datacrawl.task.BrowseCategoryTaskConf.stat=org.cld.datacrawl.task.BrsCatStat

org.cld.datacrawl.task.BrowseDetailTaskConf.entity=org.cld.datacrawl.task.BrowseDetailTaskConf
org.cld.datacrawl.task.BrowseDetailTaskConf.stat=org.cld.datacrawl.task.BrsDetailStat

org.cld.datacrawl.task.BrowseProductTaskConf.entity=org.cld.datacrawl.task.BrowseProductTaskConf
org.cld.datacrawl.task.BrowseProductTaskConf.stat=org.cld.taskmgr.entity.TaskStat

org.cld.datacrawl.task.TestTaskConf.entity=org.cld.datacrawl.task.TestTaskConf
org.cld.datacrawl.task.TestTaskConf.stat=org.cld.taskmgr.entity.TaskStat

org.cld.datacrawl.task.InvokeTaskTaskConf.entity=org.cld.datacrawl.task.InvokeTaskTaskConf
org.cld.datacrawl.task.InvokeTaskTaskConf.stat=org.cld.taskmgr.entity.TaskStat

#task definitions
#task.name=szse-stock-basic.xml, hkse-stock-basic.xml, shse-stock-basic.xml
task.name=