tconf.type=org.cld.datacrawl.CrawlConf
#task mgr config
hadoop.hdfs.default.name=hdfs://192.85.247.104:19000
hdfs.task.folder:/reminder/task
hdfs.crawleditem.folder:/reminder/items
hadoop.job.tracker:192.85.247.104:9001
task.mapper.class:org.cld.datacrawl.hadoop.CrawlTaskMapper
mapreduce.jobhistory.address=192.85.247.104:10020
mapreduce.jobhistory.webapp.address=192.85.247.104:19888
mapreduce.jobhistory.intermediate-done-dir=/mr-history/tmp
mapreduce.jobhistory.done-dir=/mr-history/done
mapred.textoutputformat.separator=,
mapreduce.task.timeout=0
mapreduce.job.split.metainfo.maxsize=-1
mapreduce.map.speculative=false
dfs.replication=1
yarn.application.classpath=/reminder/lib/
mapreduce.job.user.classpath.first=true
crawl.task.per.mapper=20

#crawl mgr config
use.proxy=true
#houston proxy
proxy.ip=16.85.88.10
proxy.port=8080

#time out for remote request, fetch page, invoke ws request, etc, in second unit
time.out=100

#for crawl
# of timeout retry for each link = retry.num * max.loop * wait.time
retry.num=10
max.loop=5
wait.time=1000
#
ws.main.url=http://52.1.96.115:8080/crbookws/services/crbookrs
#ws.main.url=http://localhost:8080/crbookws/services/crbookrs

crawl.ds.manager=hbase

#product definition
product.type=book,default
book.entity.impl=org.cld.util.entity.Product
book.handler.impl=org.cld.crbook.util.BookHandler
default.entity.impl=org.cld.util.entity.Product
default.handler.impl=org.cld.pagea.general.DefaultHandler

#task type definition
task.type=org.cld.datacrawl.task.BrowseProductTaskConf

org.cld.datacrawl.task.BrowseProductTaskConf.entity=org.cld.datacrawl.task.BrowseProductTaskConf
org.cld.datacrawl.task.BrowseProductTaskConf.stat=org.cld.taskmgr.entity.TaskStat


#task definitions
task.name=